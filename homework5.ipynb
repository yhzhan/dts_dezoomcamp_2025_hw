{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a0938c2-82f2-4a08-b1d7-fb30a3e730b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.5.5\n",
      "      /_/\n",
      "                        \n",
      "Using Scala version 2.12.18, OpenJDK 64-Bit Server VM, 11.0.26\n",
      "Branch HEAD\n",
      "Compiled by user ubuntu on 2025-02-23T20:30:46Z\n",
      "Revision 7c29c664cdc9321205a98a14858aaf8daaa19db2\n",
      "Url https://github.com/apache/spark\n",
      "Type --help for more information.\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "!spark-shell --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d73c7139-2872-4b6d-8054-5ec9ca4a7019",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/08 23:32:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The PySpark 3.5.5 version is running...\n"
     ]
    }
   ],
   "source": [
    "# Create SparkSession\n",
    "spark = SparkSession.builder.master(\"local[1]\") \\\n",
    "                    .appName('test-spark') \\\n",
    "                    .getOrCreate()\n",
    "\n",
    "print(f'The PySpark {spark.version} version is running...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d278ca3-3dbb-4ffc-ba07-5205b3835747",
   "metadata": {},
   "source": [
    "## 2. Yellow October 2024\n",
    "\n",
    "Read the October 2024 Yellow into a Spark Dataframe.\n",
    "\n",
    "Repartition the Dataframe to 4 partitions and save it to parquet.\n",
    "\n",
    "What is the average size of the Parquet (ending with .parquet extension) Files that were created (in MB)? Select the answer which most closely matches.\n",
    "\n",
    "* 6MB\n",
    "* 25MB\n",
    "* 75MB\n",
    "* 100MB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9808cbf9-bd0b-41cd-96c6-98471b6bf52c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp_ntz (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp_ntz (nullable = true)\n",
      " |-- passenger_count: long (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: long (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: long (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- Airport_fee: double (nullable = true)\n",
      "\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|Airport_fee|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|       2| 2024-10-01 00:30:44|  2024-10-01 00:48:26|              1|          3.0|         1|                 N|         162|         246|           1|       18.4|  1.0|    0.5|       1.5|         0.0|                  1.0|        24.9|                 2.5|        0.0|\n",
      "|       1| 2024-10-01 00:12:20|  2024-10-01 00:25:25|              1|          2.2|         1|                 N|          48|         236|           1|       14.2|  3.5|    0.5|       3.8|         0.0|                  1.0|        23.0|                 2.5|        0.0|\n",
      "|       1| 2024-10-01 00:04:46|  2024-10-01 00:13:52|              1|          2.7|         1|                 N|         142|          24|           1|       13.5|  3.5|    0.5|       3.7|         0.0|                  1.0|        22.2|                 2.5|        0.0|\n",
      "|       1| 2024-10-01 00:12:10|  2024-10-01 00:23:01|              1|          3.1|         1|                 N|         233|          75|           1|       14.2|  3.5|    0.5|       2.0|         0.0|                  1.0|        21.2|                 2.5|        0.0|\n",
      "|       1| 2024-10-01 00:30:22|  2024-10-01 00:30:39|              1|          0.0|         1|                 N|         262|         262|           3|        3.0|  3.5|    0.5|       0.0|         0.0|                  1.0|         8.0|                 2.5|        0.0|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(\"yellow_tripdata_2024-10.parquet\")\n",
    "\n",
    "df.printSchema()\n",
    "df.show(5)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7121d778-db26-4308-82ea-d3618dbdcba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df.repartition(4).write.mode(\"overwrite\").parquet(\"yellow_tripdata_output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c4f367b-f477-486f-b218-be92b18a394f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Parquet File Size: 23.79 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# get all the parquet files\n",
    "parquet_files = [f for f in os.listdir(\"yellow_tripdata_output\") if f.endswith(\".parquet\")]\n",
    "file_sizes = [os.path.getsize(f\"yellow_tripdata_output/{f}\") for f in parquet_files]\n",
    "\n",
    "# calculate the average size\n",
    "average_size_mb = sum(file_sizes) / len(file_sizes) / (1024 * 1024)\n",
    "print(f\"Average Parquet File Size: {average_size_mb:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4142b5-467b-43b4-898c-30a434e1ce29",
   "metadata": {},
   "source": [
    "## 3. Cound records\n",
    "How many taxi trips were there on the 15th of October?\n",
    "\n",
    "Consider only trips that started on the 15th of October.\n",
    "\n",
    "* 85,567\n",
    "* 105,567\n",
    "* 125,567\n",
    "* 145,567\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52892579-db12-41d5-9dd2-424d51d95c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trips on 2024-10-15: 128893\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, to_date\n",
    "\n",
    "trip_count = df.filter(to_date(col(\"tpep_pickup_datetime\")) == \"2024-10-15\").count()\n",
    "\n",
    "print(f\"Total trips on 2024-10-15: {trip_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfec4f91-8a44-4284-a526-c9cc825fdd67",
   "metadata": {},
   "source": [
    "## 4. Longest trip\n",
    "What is the length of the longest trip in the dataset in hours?\n",
    "\n",
    "* 122\n",
    "* 142\n",
    "* 162\n",
    "* 182"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc814bd8-e88c-4688-b355-e70dac425234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest trip duration: 162.62 hours\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, unix_timestamp\n",
    "\n",
    "df_with_duration = df.withColumn(\n",
    "    \"trip_duration_hours\",\n",
    "    (unix_timestamp(col(\"tpep_dropoff_datetime\")) - unix_timestamp(col(\"tpep_pickup_datetime\"))) / 3600\n",
    ")\n",
    "\n",
    "max_trip_duration = df_with_duration.agg({\"trip_duration_hours\": \"max\"}).collect()[0][0]\n",
    "\n",
    "print(f\"Longest trip duration: {max_trip_duration:.2f} hours\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f65ec1-37b4-4021-9acc-d964c0230986",
   "metadata": {},
   "source": [
    "## 6: Least frequent pickup location zone\n",
    "Load the zone lookup data into a temp view in Spark:\n",
    "\n",
    "wget https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv\n",
    "Using the zone lookup data and the Yellow October 2024 data, what is the name of the LEAST frequent pickup location Zone?\n",
    "\n",
    "* Governor's Island/Ellis Island/Liberty Island\n",
    "* Arden Heights\n",
    "* Rikers Island\n",
    "* Jamaica Bay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c672306-059c-495a-aedb-40d26ed7f76f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-03-08 23:45:49--  https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv\n",
      "Resolving d37ci6vzurychx.cloudfront.net (d37ci6vzurychx.cloudfront.net)... 2600:9000:2759:2c00:b:20a5:b140:21, 2600:9000:2759:9400:b:20a5:b140:21, 2600:9000:2759:be00:b:20a5:b140:21, ...\n",
      "Connecting to d37ci6vzurychx.cloudfront.net (d37ci6vzurychx.cloudfront.net)|2600:9000:2759:2c00:b:20a5:b140:21|:443... connected.\n",
      "200 OKequest sent, awaiting response... \n",
      "Length: 12331 (12K) [text/csv]\n",
      "Saving to: ‘taxi_zone_lookup.csv’\n",
      "\n",
      "taxi_zone_lookup.cs 100%[===================>]  12,04K  --.-KB/s    in 0s      \n",
      "\n",
      "2025-03-08 23:45:49 (3,83 GB/s) - ‘taxi_zone_lookup.csv’ saved [12331/12331]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05f7380b-40f2-46ca-904f-5936b8bc17b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- LocationID: string (nullable = true)\n",
      " |-- Borough: string (nullable = true)\n",
      " |-- Zone: string (nullable = true)\n",
      " |-- service_zone: string (nullable = true)\n",
      "\n",
      "+----------+-------------+--------------------+------------+\n",
      "|LocationID|      Borough|                Zone|service_zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "|         1|          EWR|      Newark Airport|         EWR|\n",
      "|         2|       Queens|         Jamaica Bay|   Boro Zone|\n",
      "|         3|        Bronx|Allerton/Pelham G...|   Boro Zone|\n",
      "|         4|    Manhattan|       Alphabet City| Yellow Zone|\n",
      "|         5|Staten Island|       Arden Heights|   Boro Zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_zones = spark.read.option(\"header\", \"true\").csv(\"taxi_zone_lookup.csv\")\n",
    "df_zones.createOrReplaceTempView(\"zones\")\n",
    "df_zones.printSchema()\n",
    "df_zones.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2fc1f708-9056-4a12-8ed6-0ac629ed7a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Least frequent pickup location zone: Governor's Island/Ellis Island/Liberty Island\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "pickup_counts = df.groupBy(\"PULocationID\").count()\n",
    "pickup_counts_with_zone = pickup_counts.join(df_zones, pickup_counts.PULocationID == df_zones.LocationID, \"left\").select(\"Zone\", \"count\").orderBy(col(\"count\").asc())\n",
    "least_frequent_zone = pickup_counts_with_zone.limit(1).collect()[0][\"Zone\"]\n",
    "print(f\"Least frequent pickup location zone: {least_frequent_zone}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fb5da1-241f-4e7c-b6e9-4081d89caf2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
